{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.compose import (\n",
    "    ColumnEnsembleClassifier,\n",
    "    TimeSeriesForestClassifier,\n",
    ")\n",
    "from sktime.classification.dictionary_based import BOSSEnsemble\n",
    "from sktime.transformations.panel.compose import ColumnConcatenator\n",
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for dirname, _, filenames in os.walk('../data/raw/second_version/data_train/data_train/'):\n",
    "    for filename in filenames:\n",
    "        files.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_list_of_files = [file.split('/')[-1] for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/raw/second_version/train.csv', \n",
    "                index_col='id').loc[ordered_list_of_files].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = []\n",
    "for file in files:\n",
    "    list_of_cols = [pd.Series(col) for col in pd.read_csv(file).T.values]\n",
    "    list_of_cols.append(pd.read_csv(file).T.values[3] / \n",
    "                        pd.read_csv(file).T.values[2])\n",
    "    list_of_cols.append(pd.read_csv(file).T.values[3] / \n",
    "                        pd.read_csv(file).T.values[1])\n",
    "    list_of_cols.append(pd.read_csv(file).T.values[3] / \n",
    "                        pd.read_csv(file).T.values[0])\n",
    "    list_of_dfs.append(list_of_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1575, 7) (1575,) (525, 7) (525,)\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame(list_of_dfs)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series concatenation + TimeSeriesForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , class_weight=dict(zip([1,2,3,4],(pd.Series(y).value_counts()/len(y)).values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9695238095238096"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", TimeSeriesForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# from sklearn.metrics import classification_report\n",
    "# print('\\nClassification Report\\n')\n",
    "# print(classification_report(y_test, y_pred, target_names=['Class 1', 'Class 2', 'Class 3', 'Class 4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series concatenation + ColumnEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-f878186f3773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m steps = [\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"concatenate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnConcatenator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m\"classify\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumnEnsembleClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'estimators'"
     ]
    }
   ],
   "source": [
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", ColumnEnsembleClassifier()),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-5d7ee41d7b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ]\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hacks/hackatom/power-transformer-anomaly-detection/ml-project-venv/lib/python3.8/site-packages/sktime/classification/compose/_column_ensemble.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplace_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hacks/hackatom/power-transformer-anomaly-detection/ml-project-venv/lib/python3.8/site-packages/sktime/classification/dictionary_based/_boss.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0mboss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shorten_bags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     boss.accuracy = self._individual_train_acc(\n\u001b[0m\u001b[1;32m    182\u001b[0m                         \u001b[0mboss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc_for_win_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     )\n",
      "\u001b[0;32m~/Desktop/Hacks/hackatom/power-transformer-anomaly-detection/ml-project-venv/lib/python3.8/site-packages/sktime/classification/dictionary_based/_boss.py\u001b[0m in \u001b[0;36m_individual_train_acc\u001b[0;34m(boss, y, train_size, lowest_acc)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hacks/hackatom/power-transformer-anomaly-detection/ml-project-venv/lib/python3.8/site-packages/sktime/classification/dictionary_based/_boss.py\u001b[0m in \u001b[0;36m_train_predict\u001b[0;34m(self, train_num)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboss_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_bag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hacks/hackatom/power-transformer-anomaly-detection/ml-project-venv/lib/python3.8/site-packages/sktime/classification/dictionary_based/_boss.py\u001b[0m in \u001b[0;36mboss_distance\u001b[0;34m(first, second, best_dist)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mval_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_a\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mdist\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = ColumnEnsembleClassifier(\n",
    "    estimators=[\n",
    "        (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n",
    "        (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=3), [3]),\n",
    "    ]\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8704761904761905"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [\n",
    "    (\"concatenate\", ColumnConcatenator()),\n",
    "    (\"classify\", RandomIntervalSpectralForest(random_state=42, n_jobs=-1)),\n",
    "]\n",
    "clf = Pipeline(steps)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with TSFRESH and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 5/5 [08:18<00:00, 99.78s/it] \n",
      "Feature Extraction: 100%|██████████| 5/5 [02:43<00:00, 32.63s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9142857142857143"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    TSFreshFeatureExtractor(show_warnings=False), \n",
    "    RandomForestClassifier(n_jobs=-1, min_samples_split=10, random_state=42, class_weight=dict(zip([1,2,3,4],(pd.Series(y).value_counts()/len(y)).values)))\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with TSFRESH and catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = TSFreshFeatureExtractor(n_jobs=-1)\n",
    "# extracted_features = transformer.fit_transform(X_train)\n",
    "# extracted_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 70/70 [03:33<00:00,  3.05s/it]\n",
      "Feature Extraction: 100%|██████████| 70/70 [01:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9676190476190476"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    TSFreshFeatureExtractor(n_jobs=-1, show_warnings=False), \n",
    "    CatBoostClassifier(random_seed=42, \n",
    "                       verbose=False,)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with TSFRESH and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 70/70 [03:33<00:00,  3.05s/it]\n",
      "Feature Extraction: 100%|██████████| 70/70 [01:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9676190476190476"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    TSFreshFeatureExtractor(n_jobs=-1, show_warnings=False), \n",
    "    XG(random_seed=42, \n",
    "                       verbose=False,)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with TSFRESH and catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = TSFreshFeatureExtractor(n_jobs=-1)\n",
    "# extracted_features = transformer.fit_transform(X_train)\n",
    "# extracted_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 70/70 [03:33<00:00,  3.05s/it]\n",
      "Feature Extraction: 100%|██████████| 70/70 [01:13<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9676190476190476"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(\n",
    "    TSFreshFeatureExtractor(n_jobs=-1, show_warnings=False), \n",
    "    CatBoostClassifier(random_seed=42, \n",
    "                       verbose=False,)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = []\n",
    "for dirname, _, filenames in os.walk('../data/raw/second_version/data_test/data_test/'):\n",
    "    for filename in filenames:\n",
    "        files.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = []\n",
    "for file in files:\n",
    "    list_of_cols = [pd.Series(col) for col in pd.read_csv(file).T.values]\n",
    "    list_of_cols.append(pd.read_csv(file).T.values[3] / \n",
    "                        pd.read_csv(file).T.values[2])\n",
    "    list_of_cols.append(pd.read_csv(file).T.values[3] / \n",
    "                        pd.read_csv(file).T.values[1])\n",
    "    list_of_cols.append(pd.read_csv(file).T.values[3] / \n",
    "                        pd.read_csv(file).T.values[0])\n",
    "    list_of_dfs.append(list_of_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = pd.DataFrame(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 70/70 [01:41<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "y_val = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'id':[file.split('/')[-1] for file in files],\n",
    " 'category':y_val[:,0]\n",
    "}).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2_trans_1.csv</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_trans_10.csv</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_trans_1003.csv</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_trans_1004.csv</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_trans_1005.csv</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  category\n",
       "id                        \n",
       "2_trans_1.csv            3\n",
       "2_trans_10.csv           1\n",
       "2_trans_1003.csv         1\n",
       "2_trans_1004.csv         3\n",
       "2_trans_1005.csv         1"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('../../submission10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
